{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eac9e08-e588-49f9-9d10-3e47328ab99d",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d73f634b-54e5-4cd8-864e-083ba84c4884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Busss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Busss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, string\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import string\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from gensim import corpora, models\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48dc676-3442-485e-9310-cab334f1c782",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1e0fda93-333c-478d-8aed-77d70e013f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Busss\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (10,11) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(204135, 12)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/data.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "18093572-0e87-4a83-b003-0a33df92c550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>publication</th>\n",
       "      <th>category</th>\n",
       "      <th>digital</th>\n",
       "      <th>section</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Agent Cooper in Twin Peaks is the audience: on...</td>\n",
       "      <td>\\nTasha Robinson\\n</td>\n",
       "      <td>2017-05-31</td>\n",
       "      <td>And never more so than in Showtime’s new...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>AI, the humanity!</td>\n",
       "      <td>\\nSam Byford\\n</td>\n",
       "      <td>2017-05-30</td>\n",
       "      <td>AlphaGo’s victory isn’t a defeat for hum...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The Viral Machine</td>\n",
       "      <td>\\nKaitlyn Tiffany\\n</td>\n",
       "      <td>2017-05-25</td>\n",
       "      <td>Super Deluxe built a weird internet empi...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>How Anker is beating Apple and Samsung at thei...</td>\n",
       "      <td>\\nNick Statt\\n</td>\n",
       "      <td>2017-05-22</td>\n",
       "      <td>Steven Yang quit his job at Google in th...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Tour Black Panther’s reimagined homeland with ...</td>\n",
       "      <td>\\nKwame Opam\\n</td>\n",
       "      <td>2017-05-15</td>\n",
       "      <td>Ahead of Black Panther’s 2018 theatrical...</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Verge</td>\n",
       "      <td>Longform</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title               author  \\\n",
       "0   1  Agent Cooper in Twin Peaks is the audience: on...   \\nTasha Robinson\\n   \n",
       "1   2                                  AI, the humanity!       \\nSam Byford\\n   \n",
       "2   3                                  The Viral Machine  \\nKaitlyn Tiffany\\n   \n",
       "3   4  How Anker is beating Apple and Samsung at thei...       \\nNick Statt\\n   \n",
       "4   5  Tour Black Panther’s reimagined homeland with ...       \\nKwame Opam\\n   \n",
       "\n",
       "         date                                            content    year  \\\n",
       "0  2017-05-31        And never more so than in Showtime’s new...  2017.0   \n",
       "1  2017-05-30        AlphaGo’s victory isn’t a defeat for hum...  2017.0   \n",
       "2  2017-05-25        Super Deluxe built a weird internet empi...  2017.0   \n",
       "3  2017-05-22        Steven Yang quit his job at Google in th...  2017.0   \n",
       "4  2017-05-15        Ahead of Black Panther’s 2018 theatrical...  2017.0   \n",
       "\n",
       "   month publication  category  digital section  url  \n",
       "0    5.0       Verge  Longform      1.0     NaN  NaN  \n",
       "1    5.0       Verge  Longform      1.0     NaN  NaN  \n",
       "2    5.0       Verge  Longform      1.0     NaN  NaN  \n",
       "3    5.0       Verge  Longform      1.0     NaN  NaN  \n",
       "4    5.0       Verge  Longform      1.0     NaN  NaN  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eac3be-65dc-4645-9f89-00dc79b06ae8",
   "metadata": {},
   "source": [
    "#### Removing all columns and keeping only the 'content' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ce011ccd-3255-45a3-93b9-5d21a3e1fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = [i for i in df.columns if i!='title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eb5e3b0f-6863-4dfa-9c81-71036a2f7d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agent Cooper in Twin Peaks is the audience: on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI, the humanity!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Viral Machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How Anker is beating Apple and Samsung at thei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tour Black Panther’s reimagined homeland with ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title\n",
       "0  Agent Cooper in Twin Peaks is the audience: on...\n",
       "1                                  AI, the humanity!\n",
       "2                                  The Viral Machine\n",
       "3  How Anker is beating Apple and Samsung at thei...\n",
       "4  Tour Black Panther’s reimagined homeland with ..."
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(remove, axis = 1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "41bc6857-febf-4ad2-a89c-b573467f2455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204135, 1)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bfc19694-5938-40b5-bc79-f95a6c5d5a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 204135 entries, 0 to 204134\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   title   204132 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dca478f2-858b-4d83-a8a0-a7e862bf6f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "acdee504-d88f-408e-a825-79a96146ce19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204132, 1)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9405f63-a3e2-496a-931b-283f8fe3e28e",
   "metadata": {},
   "source": [
    "#### Text preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9e70ce6d-010d-400e-9408-e88dd835be82",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def lem_stem(word):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(word))\n",
    "\n",
    "def preprocess(text):\n",
    "    return [lem_stem(token) for token in gensim.utils.simple_preprocess(text) if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3]            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58a60ca-689d-4467-ad80-341c32b463e4",
   "metadata": {},
   "source": [
    "#### Checking the functions at work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "00048bc5-71f3-4f6c-abb2-544aeaae25eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: How Anker is beating Apple and Samsung at their own accessory game\n",
      "after: ['anker', 'beat', 'appl', 'samsung', 'accessori', 'game']\n"
     ]
    }
   ],
   "source": [
    "print('original:',df['title'].iloc[3][:100])\n",
    "print(\"after:\",preprocess(df['title'].iloc[3][:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a418e5-bf55-4a2a-91ba-30bd52a66d05",
   "metadata": {},
   "source": [
    "#### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c980445-091b-49f5-bdfa-a5edbabfcb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_data = df['title'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f02f88d8-93fd-4f89-b323-21157725904c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [agent, cooper, twin, peak, audienc, delight, ...\n",
       "1                                              [human]\n",
       "2                                      [viral, machin]\n",
       "3        [anker, beat, appl, samsung, accessori, game]\n",
       "4    [tour, black, panther, reimagin, homeland, neh...\n",
       "5                                     [instant, recal]\n",
       "6                                     [massiv, attack]\n",
       "7                                       [futur, agenc]\n",
       "8                                       [flight, risk]\n",
       "9                                       [brain, drain]\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aa2a88-2171-45d3-85a5-9df8830d68ac",
   "metadata": {},
   "source": [
    "#### Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "71f083a3-0b5a-4663-85b8-8cf2f70da3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_words = gensim.corpora.Dictionary(pp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e6a9ba83-9bf9-4000-ac09-5df90fa6714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 agent\n",
      "1 audienc\n",
      "2 cooper\n",
      "3 delight\n",
      "4 disintegr\n",
      "5 peak\n",
      "6 twin\n",
      "7 human\n",
      "8 machin\n",
      "9 viral\n",
      "10 accessori\n"
     ]
    }
   ],
   "source": [
    "for i, (key, value) in enumerate(dict_of_words.iteritems()):\n",
    "    print(key, value)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59189f2b-6467-441b-bd7d-7433b7ee0c8b",
   "metadata": {},
   "source": [
    "#### Total number of words in dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad54185c-1d97-4b55-aea6-4d3d82301b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 32165\n"
     ]
    }
   ],
   "source": [
    "print('total:',len(dict_of_words.iteritems()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977baaab-0e98-4fab-a15c-f061d560da6a",
   "metadata": {},
   "source": [
    "#### Filter out tokens\n",
    "remove < 'no_below', \n",
    "remove > 'no_above',\n",
    "then keep only the first 'keep_n' most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dd76f273-435c-4bd5-9118-294312b87ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_words.filter_extremes(no_below=15, no_above=0.1, keep_n=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b258c5ac-b062-4f7d-b494-705b2738865f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)],\n",
       " [(6, 1)],\n",
       " [(7, 1), (8, 1)],\n",
       " [(9, 1), (10, 1), (11, 1), (12, 1), (13, 1)],\n",
       " [(14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1)],\n",
       " [(21, 1), (22, 1)],\n",
       " [(23, 1), (24, 1)],\n",
       " [(25, 1), (26, 1)],\n",
       " [(27, 1), (28, 1)],\n",
       " [(29, 1), (30, 1)]]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_for_docs = [dict_of_words.doc2bow(i) for i in pp_data]\n",
    "bow_for_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897eb4e6-1808-42b5-888f-d6f8b2c427d3",
   "metadata": {},
   "source": [
    "#### How many words in the title at the i-th location\n",
    "(i==2 in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7ceb044-71a3-49d7-9f2a-78e307538e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_for_docs[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba6967d-c6d8-4a43-94fc-06be9cb2baa7",
   "metadata": {},
   "source": [
    "#### Check the words at that location\n",
    "(id, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56b6d57-bb40-414c-b97f-71cb5d84ce3e",
   "metadata": {},
   "source": [
    "##### Ordered by how they appear in the content - top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3747306-ad8b-453e-8617-51b5836e63e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 1), (10, 1), (11, 1), (12, 1), (13, 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_for_docs[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3deeadb-23a4-4a89-8517-0504d8b9135b",
   "metadata": {},
   "source": [
    "##### Ordered by the most frequent - top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62ea8553-7512-48af-8c85-760d2a249e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 1), (10, 1), (11, 1), (12, 1), (13, 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(bow_for_docs[3],  key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39bfd36-f1f3-4049-857b-4784e741e799",
   "metadata": {},
   "source": [
    "##### Ordered by the least frequent - top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13473caa-acec-4e87-9113-c9d5ee3620dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 1), (10, 1), (11, 1), (12, 1), (13, 1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(bow_for_docs[3],  key=lambda x: x[1])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8839d9-4b77-44f3-b479-0ba8dd549444",
   "metadata": {},
   "source": [
    "#### Check the actual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0125b030-0d82-47c3-ab9c-6f968fc52f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOC | WORD NUMBER |  WORD FREQ  | THE ACTUAL WORD\n",
      "--------------------------------------------------\n",
      "  0 |       9     |       1     |     accessori\n",
      "  1 |      10     |       1     |     appl\n",
      "  2 |      11     |       1     |     beat\n",
      "  3 |      12     |       1     |     game\n",
      "  4 |      13     |       1     |     samsung\n",
      "[(9, 1), (10, 1), (11, 1), (12, 1), (13, 1)]\n"
     ]
    }
   ],
   "source": [
    "limit = 10\n",
    "space = '    '\n",
    "tmp_words = bow_for_docs[3]\n",
    "\n",
    "print(f'LOC | WORD NUMBER |  WORD FREQ  | THE ACTUAL WORD')\n",
    "print('--------------------------------------------------')\n",
    "for i in range(len(tmp_words)):\n",
    "    num = str(tmp_words[i][0])\n",
    "    while len(num) < 3: num = ' '+num\n",
    "        \n",
    "    freq = str(tmp_words[i][1])\n",
    "    while len(freq) < 3: freq = ' '+freq\n",
    "\n",
    "    loc = str(i)\n",
    "    while len(loc) < 3: loc = ' '+loc\n",
    "\n",
    "    print(f'{loc} | {space}{num}{space} | {space}{freq}{space} | {space}{dict_of_words[tmp_words[i][0]]}')\n",
    "    if i == limit: break\n",
    "print(tmp_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ee3c7-c5c3-42c8-831e-5b9c01e6e5a9",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3d32fff4-476a-49f5-b7bd-61939e656672",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = models.TfidfModel(bow_for_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c35066-dff7-46ca-8174-955c10ff8ddc",
   "metadata": {},
   "source": [
    "##### Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a23eaa9a-3e20-45fe-9834-d9c4211def9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x1c7c1a01100>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf_data = idf[bow_for_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0e4bc9-6f8b-4e12-851a-4159b13cdd5f",
   "metadata": {},
   "source": [
    "##### (id, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "832efc36-4c7c-42d8-9557-0468e45be432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.3480650151787403), (1, 0.4209161975451784), (2, 0.4047524798643647), (3, 0.46501661778648373), (4, 0.4151525454802232), (5, 0.3863465271748787)]\n"
     ]
    }
   ],
   "source": [
    "for i in tdf_data:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d464c147-f9f3-47cf-8f2f-0dd0e2762540",
   "metadata": {},
   "source": [
    "### LDA\n",
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8142214a-bb88-4000-aae6-6a718cc329f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.LdaMulticore(bow_for_docs, num_topics=5, id2word=dict_of_words, passes=10, workers=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34cec3d-0030-494c-8e09-a6ddadd49eb9",
   "metadata": {},
   "source": [
    "##### Lets view the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1f92d068-9ee2-401a-99a5-3bd1099a19fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.013*\"hous\" + 0.012*\"say\" + 0.011*\"republican\" + 0.010*\"breitbart\" + 0.010*\"white\" + 0.010*\"obama\" + 0.009*\"russia\" + 0.007*\"plan\" + 0.007*\"health\" + 0.007*\"senat\"\n",
      "---\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.011*\"life\" + 0.008*\"american\" + 0.008*\"world\" + 0.008*\"opinion\" + 0.008*\"dy\" + 0.007*\"video\" + 0.007*\"review\" + 0.006*\"home\" + 0.005*\"year\" + 0.005*\"histori\"\n",
      "---\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.029*\"opinion\" + 0.013*\"america\" + 0.012*\"breitbart\" + 0.011*\"north\" + 0.010*\"korea\" + 0.010*\"presid\" + 0.009*\"donald\" + 0.009*\"woman\" + 0.008*\"week\" + 0.007*\"brief\"\n",
      "---\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.015*\"kill\" + 0.015*\"attack\" + 0.012*\"polic\" + 0.011*\"court\" + 0.009*\"state\" + 0.008*\"case\" + 0.008*\"death\" + 0.008*\"offic\" + 0.008*\"shoot\" + 0.007*\"say\"\n",
      "---\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.027*\"clinton\" + 0.015*\"donald\" + 0.014*\"hillari\" + 0.010*\"like\" + 0.010*\"breitbart\" + 0.007*\"look\" + 0.007*\"year\" + 0.007*\"wall\" + 0.005*\"win\" + 0.005*\"appl\"\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, topic in lda.print_topics(-1):\n",
    "    print(f\"Topic: {i} \\nWords: {topic}\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076891b9-e7bf-40d3-88b9-821bdd774fcd",
   "metadata": {},
   "source": [
    "#### Topic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1726bf40-9a41-454b-96f3-46a79be85b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Topic: 0 Words: 0.015*\"player\" + 0.012*\"season\" + 0.009*\"sport\" + 0.008*\"leagu\" + 0.007*\"coach\" + 0.007*\"olymp\" + 0.006*\"athlet\" + 0.005*\"footbal\" + 0.005*\"field\" + 0.005*\"ball\"'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Topic: 0 Words: 0.015*\"player\" + 0.012*\"season\" + 0.009*\"sport\" + 0.008*\"leagu\" + 0.007*\"coach\" + 0.007*\"olymp\" + 0.006*\"athlet\" + 0.005*\"footbal\" + 0.005*\"field\" + 0.005*\"ball\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4779fdc-26be-4e55-9dbf-a1297e1016cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68b724e2-a1c3-4d45-9e0c-2fe537056c27",
   "metadata": {},
   "source": [
    "### LDA + TF-IDF\n",
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8cda9c0e-dc97-49c5-8806-9ef5d6303979",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_idf = gensim.models.LdaMulticore(tdf_data, num_topics=5, id2word=dict_of_words, passes=15, workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e1b6466f-90d9-4766-a919-10f53220c970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.004*\"home\" + 0.004*\"opinion\" + 0.004*\"appl\" + 0.004*\"wall\" + 0.004*\"street\" + 0.004*\"world\" + 0.003*\"sale\" + 0.003*\"year\" + 0.003*\"record\" + 0.003*\"crash\"\n",
      "---\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.007*\"north\" + 0.007*\"opinion\" + 0.006*\"korea\" + 0.006*\"court\" + 0.005*\"say\" + 0.005*\"state\" + 0.005*\"attack\" + 0.005*\"syria\" + 0.004*\"china\" + 0.004*\"deal\"\n",
      "---\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.006*\"dy\" + 0.005*\"year\" + 0.005*\"woman\" + 0.005*\"life\" + 0.005*\"polic\" + 0.004*\"review\" + 0.004*\"shoot\" + 0.004*\"star\" + 0.004*\"kill\" + 0.004*\"death\"\n",
      "---\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.012*\"clinton\" + 0.010*\"opinion\" + 0.008*\"donald\" + 0.008*\"republican\" + 0.007*\"hillari\" + 0.007*\"democrat\" + 0.007*\"health\" + 0.006*\"breitbart\" + 0.006*\"read\" + 0.006*\"presid\"\n",
      "---\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.018*\"opinion\" + 0.005*\"brief\" + 0.005*\"book\" + 0.004*\"news\" + 0.004*\"america\" + 0.004*\"even\" + 0.004*\"facebook\" + 0.004*\"thing\" + 0.004*\"olymp\" + 0.004*\"breitbart\"\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, topic in lda_idf.print_topics(-1):\n",
    "    print(f\"Topic: {i} \\nWords: {topic}\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b7df00-1341-4af3-ad10-e17fc790de88",
   "metadata": {},
   "source": [
    "#### Topic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7b283e9d-5ef6-4eb6-9dfd-294bf267114a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Topic: 0 Words: 0.010*\"olymp\" + 0.009*\"yanke\" + 0.008*\"player\" + 0.007*\"athlet\" + 0.007*\"met\" + 0.006*\"cuban\" + 0.006*\"cuba\" + 0.006*\"basebal\" + 0.006*\"leagu\" + 0.006*\"season\"'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Topic: 0 Words: 0.010*\"olymp\" + 0.009*\"yanke\" + 0.008*\"player\" + 0.007*\"athlet\" + 0.007*\"met\" + 0.006*\"cuban\" + 0.006*\"cuba\" + 0.006*\"basebal\" + 0.006*\"leagu\" + 0.006*\"season\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f8f12-f978-44ef-b41d-03d7d4caa0ef",
   "metadata": {},
   "source": [
    "### Testing\n",
    "###### Using these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2579df64-22bd-4d3c-b32f-216a52180dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['watch', 'histori', 'movi', 'trailer', 'told', 'star', 'war']\n"
     ]
    }
   ],
   "source": [
    "loc = 321\n",
    "tmp_doc = pp_data[loc]\n",
    "print(tmp_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d521d536-6224-4cc3-a856-187c8b8ced59",
   "metadata": {},
   "source": [
    "###### Using this article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7b2cd6e1-e67f-4b2d-a268-e6afb3249245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: Watch the history of the movie trailer, as told by Star Wars\n"
     ]
    }
   ],
   "source": [
    "print('original:',df['title'].iloc[loc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4491a3d2-9e61-4293-af2b-3159c50173e9",
   "metadata": {},
   "source": [
    "#### LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5320b580-31b8-40fa-a79c-38be605450d8",
   "metadata": {},
   "source": [
    "##### Lets check which topic 'tmp_doc' above belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0727164b-faf9-4537-a1e9-ff01c2555c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.729004442691803\t \n",
      "Topic: 0.029*\"opinion\" + 0.013*\"america\" + 0.012*\"breitbart\" + 0.011*\"north\" + 0.010*\"korea\" + 0.010*\"presid\" + 0.009*\"donald\" + 0.009*\"woman\" + 0.008*\"week\" + 0.007*\"brief\"\n",
      "\n",
      "Score: 0.1957699954509735\t \n",
      "Topic: 0.011*\"life\" + 0.008*\"american\" + 0.008*\"world\" + 0.008*\"opinion\" + 0.008*\"dy\" + 0.007*\"video\" + 0.007*\"review\" + 0.006*\"home\" + 0.005*\"year\" + 0.005*\"histori\"\n",
      "\n",
      "Score: 0.025080086663365364\t \n",
      "Topic: 0.015*\"kill\" + 0.015*\"attack\" + 0.012*\"polic\" + 0.011*\"court\" + 0.009*\"state\" + 0.008*\"case\" + 0.008*\"death\" + 0.008*\"offic\" + 0.008*\"shoot\" + 0.007*\"say\"\n",
      "\n",
      "Score: 0.025074927136301994\t \n",
      "Topic: 0.027*\"clinton\" + 0.015*\"donald\" + 0.014*\"hillari\" + 0.010*\"like\" + 0.010*\"breitbart\" + 0.007*\"look\" + 0.007*\"year\" + 0.007*\"wall\" + 0.005*\"win\" + 0.005*\"appl\"\n",
      "\n",
      "Score: 0.025070572271943092\t \n",
      "Topic: 0.013*\"hous\" + 0.012*\"say\" + 0.011*\"republican\" + 0.010*\"breitbart\" + 0.010*\"white\" + 0.010*\"obama\" + 0.009*\"russia\" + 0.007*\"plan\" + 0.007*\"health\" + 0.007*\"senat\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, conf in sorted(lda[bow_for_docs[loc]], key=lambda x: x[1]*-1):\n",
    "    print(f\"Score: {conf}\\t \\nTopic: {lda.print_topic(i, 10)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eafdfef-0dd9-4b37-b30a-d87a07f0d1da",
   "metadata": {},
   "source": [
    "###### After viewing the original text and the cluster/classification is 'topic 0' which is movies/arrt/review with 72% confidence the model is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039df797-1a37-4cef-be54-08428e77be97",
   "metadata": {},
   "source": [
    "### Testing LDA + TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "79fb5d59-06f1-49ba-8022-63ab182129f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.8992657661437988\t \n",
      "Topic: 0.006*\"dy\" + 0.005*\"year\" + 0.005*\"woman\" + 0.005*\"life\" + 0.005*\"polic\" + 0.004*\"review\" + 0.004*\"shoot\" + 0.004*\"star\" + 0.004*\"kill\" + 0.004*\"death\"\n",
      "\n",
      "Score: 0.025354212149977684\t \n",
      "Topic: 0.018*\"opinion\" + 0.005*\"brief\" + 0.005*\"book\" + 0.004*\"news\" + 0.004*\"america\" + 0.004*\"even\" + 0.004*\"facebook\" + 0.004*\"thing\" + 0.004*\"olymp\" + 0.004*\"breitbart\"\n",
      "\n",
      "Score: 0.025177814066410065\t \n",
      "Topic: 0.004*\"home\" + 0.004*\"opinion\" + 0.004*\"appl\" + 0.004*\"wall\" + 0.004*\"street\" + 0.004*\"world\" + 0.003*\"sale\" + 0.003*\"year\" + 0.003*\"record\" + 0.003*\"crash\"\n",
      "\n",
      "Score: 0.025153230875730515\t \n",
      "Topic: 0.012*\"clinton\" + 0.010*\"opinion\" + 0.008*\"donald\" + 0.008*\"republican\" + 0.007*\"hillari\" + 0.007*\"democrat\" + 0.007*\"health\" + 0.006*\"breitbart\" + 0.006*\"read\" + 0.006*\"presid\"\n",
      "\n",
      "Score: 0.02504894696176052\t \n",
      "Topic: 0.007*\"north\" + 0.007*\"opinion\" + 0.006*\"korea\" + 0.006*\"court\" + 0.005*\"say\" + 0.005*\"state\" + 0.005*\"attack\" + 0.005*\"syria\" + 0.004*\"china\" + 0.004*\"deal\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, conf in sorted(lda_idf[bow_for_docs[loc]], key=lambda x: x[1]*-1):\n",
    "    print(f\"Score: {conf}\\t \\nTopic: {lda_idf.print_topic(i, 10)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5dd0a8-c05f-420c-a48e-872d91ae59c9",
   "metadata": {},
   "source": [
    "###### After viewing the original and the the cluster/classification. It appears that the model is 90% confident that the topic is 'topic 0' Which is movie/review/life. That is the correct classification for the document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514e3720-04d8-47cf-a494-dfb563ee0ca0",
   "metadata": {},
   "source": [
    "### Test on new documents\n",
    "article from: https://www.arabnews.com/node/1992296/sport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dacede25-1371-4cc4-9bcb-5ba7f834a551",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = 'Ghostbusters Learned From Force Awakens Mistakes (And It Worked Perfectly)'\n",
    "bow = dict_of_words.doc2bow(preprocess(new_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcd2584-c9ae-46ad-9623-0ad4d606cefe",
   "metadata": {},
   "source": [
    "#### Testing LDA on new article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "577675ae-f2a6-48e8-959f-022c7db0d670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5589032769203186\t \n",
      "Topic: 0.011*\"life\" + 0.008*\"american\" + 0.008*\"world\" + 0.008*\"opinion\" + 0.008*\"dy\" + 0.007*\"video\" + 0.007*\"review\" + 0.006*\"home\" + 0.005*\"year\" + 0.005*\"histori\"\n",
      "\n",
      "Score: 0.24325038492679596\t \n",
      "Topic: 0.013*\"hous\" + 0.012*\"say\" + 0.011*\"republican\" + 0.010*\"breitbart\" + 0.010*\"white\" + 0.010*\"obama\" + 0.009*\"russia\" + 0.007*\"plan\" + 0.007*\"health\" + 0.007*\"senat\"\n",
      "\n",
      "Score: 0.14723505079746246\t \n",
      "Topic: 0.029*\"opinion\" + 0.013*\"america\" + 0.012*\"breitbart\" + 0.011*\"north\" + 0.010*\"korea\" + 0.010*\"presid\" + 0.009*\"donald\" + 0.009*\"woman\" + 0.008*\"week\" + 0.007*\"brief\"\n",
      "\n",
      "Score: 0.025336600840091705\t \n",
      "Topic: 0.015*\"kill\" + 0.015*\"attack\" + 0.012*\"polic\" + 0.011*\"court\" + 0.009*\"state\" + 0.008*\"case\" + 0.008*\"death\" + 0.008*\"offic\" + 0.008*\"shoot\" + 0.007*\"say\"\n",
      "\n",
      "Score: 0.02527468465268612\t \n",
      "Topic: 0.027*\"clinton\" + 0.015*\"donald\" + 0.014*\"hillari\" + 0.010*\"like\" + 0.010*\"breitbart\" + 0.007*\"look\" + 0.007*\"year\" + 0.007*\"wall\" + 0.005*\"win\" + 0.005*\"appl\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, conf in sorted(lda[bow], key=lambda x: x[1]*-1):\n",
    "    print(f\"Score: {conf}\\t \\nTopic: {lda.print_topic(i, 10)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3442aa-7d08-4211-8c23-a8442004bbf0",
   "metadata": {},
   "source": [
    "#### Result\n",
    "The LDA model has classified the aritcle correctly into review/life/movies. 55%confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35860301-bc8c-40e9-9d0e-b3065290196c",
   "metadata": {},
   "source": [
    "#### Testing LDA + TF-IDF on new article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d83639d1-a908-489a-8572-585a63347012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4485916495323181\t \n",
      "Topic: 0.006*\"dy\" + 0.005*\"year\" + 0.005*\"woman\" + 0.005*\"life\" + 0.005*\"polic\" + 0.004*\"review\" + 0.004*\"shoot\" + 0.004*\"star\" + 0.004*\"kill\" + 0.004*\"death\"\n",
      "\n",
      "Score: 0.29611530900001526\t \n",
      "Topic: 0.012*\"clinton\" + 0.010*\"opinion\" + 0.008*\"donald\" + 0.008*\"republican\" + 0.007*\"hillari\" + 0.007*\"democrat\" + 0.007*\"health\" + 0.006*\"breitbart\" + 0.006*\"read\" + 0.006*\"presid\"\n",
      "\n",
      "Score: 0.20367766916751862\t \n",
      "Topic: 0.004*\"home\" + 0.004*\"opinion\" + 0.004*\"appl\" + 0.004*\"wall\" + 0.004*\"street\" + 0.004*\"world\" + 0.003*\"sale\" + 0.003*\"year\" + 0.003*\"record\" + 0.003*\"crash\"\n",
      "\n",
      "Score: 0.026074189692735672\t \n",
      "Topic: 0.007*\"north\" + 0.007*\"opinion\" + 0.006*\"korea\" + 0.006*\"court\" + 0.005*\"say\" + 0.005*\"state\" + 0.005*\"attack\" + 0.005*\"syria\" + 0.004*\"china\" + 0.004*\"deal\"\n",
      "\n",
      "Score: 0.025541190057992935\t \n",
      "Topic: 0.018*\"opinion\" + 0.005*\"brief\" + 0.005*\"book\" + 0.004*\"news\" + 0.004*\"america\" + 0.004*\"even\" + 0.004*\"facebook\" + 0.004*\"thing\" + 0.004*\"olymp\" + 0.004*\"breitbart\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, conf in sorted(lda_idf[bow], key=lambda x: x[1]*-1):\n",
    "    print(f\"Score: {conf}\\t \\nTopic: {lda_idf.print_topic(i, 10)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db342000-941d-4e63-91ba-e0bcefe58072",
   "metadata": {},
   "source": [
    "#### Result\n",
    "The LDA+TF-IDF model has classified the aritcle correctly into review/life/movies.(45% confidence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
